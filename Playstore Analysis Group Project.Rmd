---
title: "Playstore Analysis Group Project"
author: "Zhijing Zhao, Karen Gao, Eva He, Katherine Li, Sarah Xi"
date: "10/9/2019"
output: html_document
---

&nbsp;

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(fitdistrplus)
play_orig <- read.csv("E:\\ND\\module1\\playStore5.csv")
play <- play_orig
str(play)
```

&nbsp;

&nbsp;

## Data Cleaning

```{r, warning=FALSE}
play <- play %>% 
  # remove the duplicates and keep the only line with the highest number of reviews
  # first, show the results in a decreasing order for reviews
  arrange(desc(Reviews)) %>% 
  
  # use distinct function to remove duplicate 
  # set keep_all to true to show all columns
  distinct(App,.keep_all = TRUE) %>% 
  
  # change it into the appropriate data type
  mutate(Rating = as.numeric(as.character(Rating)),
         Reviews = as.numeric(as.character(Reviews))) %>%
  
  # since we set ratings as dependent variable, we need to filter out the meaningless data with a NA rating
  # filter out those with non-numeric size
  # we find out that one line data is placed into the wrong column and need to filter it out
  # in the installs column, it has the 'free' character which should be in the Type column
  filter(Rating != "NA" & Reviews != 0 & Size != "Varies with device" & Installs != "Free") %>%
  
  # transform and group the 'installs' column 
  # remove the coma using str_replace_all function
  mutate(Installs = str_replace_all(Installs, ",", ""))

# remove the plus sign using gsub function
play$Installs <- gsub("\\+", "", play$Installs)
# change into appropriate data type
play$Installs = as.numeric(as.character(play$Installs))

# regroup the Installs into 6 categories which have equal proportion 
play$Installs[play$Installs == 1 | play$Installs == 5 | play$Installs == 10 | play$Installs == 50 | play$Installs == 100 | play$Installs == 500] <- "XXS"
play$Installs[play$Installs == 1000 | play$Installs == 5000] <- "XS"
play$Installs[play$Installs == 10000 | play$Installs == 50000] <- "S"
play$Installs[play$Installs == 100000 | play$Installs == 500000] <- "M"
play$Installs[play$Installs == 1000000] <- "L"
play$Installs[play$Installs == 5000000 | play$Installs == 10000000 | play$Installs == 50000000 | play$Installs == 100000000 | play$Installs == 500000000 | play$Installs == 1000000000] <- "XL"

play$Installs <- as.factor(play$Installs)

# remove the dollar sign in Price column
play$Price <- gsub("\\$", "", play$Price)
# change into appropriate data type
play$Price = as.numeric(as.character(play$Price))

# handle the Size column
# separate the Size into two column of the size number and unit(KB/MB)
play <- play %>% 
  # because the format is the same as xxM / xxK, so separate from the first in the right hand 
  separate(Size, c("size_num", "size_unit"), -1) 
play$size_num <- as.numeric(as.character(play$size_num))
# we use ifelse function to transform KB to MB
play$size_num <- ifelse(play$size_unit == "k",
                        round(play$size_num/1024),
                        play$size_num)
# delete the unit column
play <- play[,-6]

# double check the structure of dataset
str(play)
```


&nbsp;

&nbsp;

## Goal
We are trying to generate a predictive model for application developers to understand what factors will impact the overall rating of the application, and maybe make certain adjustments to their products. We picked rating as the dependent variable and by using some T-Test, Anova Test and linear regression model, we tested some of the possible independent variables which would affect the prediction of rating. 

&nbsp;

## T-TEST Ratomg ~ Type
For variables with two groups, we could use t-test to tell the mean difference between groups. As we want to test if there is a difference in rating between paid and free applications, we run a t test on ‘type’, which would tell the mean difference of rating for paid and free applications. 

&nbsp;

Hypothesis:

H0: There is no difference between free and paid applications on rating.

H1: There is a difference between free and paid applications on rating.

```{r}
t.test(play$Rating ~ play$Type, alternative = "two.side")
hist(play$Rating)
```
&nbsp;

Interpretation:

The absolute value of the t score (-2.7924) is greater than 1.96, which is sufficiently large because only if the coefficient is much larger than its standard error then t-value would be significant. We also have a significant p-value of 0.005422 greater than 0.05. This indicates that we can reject the null hypothesis and draw a conclusion that there is a difference between the free and paid applications on rating. Also, we can see the mean difference between groups through the sample estimates section. 


Limitation: 

Something needs to be noticed is that even though we reject the null hypothesis, but it is up to 50% chance of incorrectly rejecting the null for the population with p-value of 0.05 and up to 15% chance for a p-value of 0.01. Another limitation includes the dependent variable is not normally distributed based on the histogram.


&nbsp;

&nbsp;



## ANOVA 1 Rating ~ Installs
We want to see if there is a difference in ratings among different levels of installations. The installations are listed as “0”, “0+”, “1+”, “5+”, “10+”, “50+”, and continued in identical format till “1,000,000,000+”. There is also one input marked “free”. We first run a summary on the variable “installs” from the original dataset to see the detailed distribution of data across different levels of installations. To prepare the data for efficiency purposes, we aim to group all the installation levels into six groups with nearly equal proportion of data included. We group “0”, “0+”, “1+”, “5+”, “10+”, “50+”, “100+”, and “500+” as the first group which contains 979 data points, and assign “XXS” to data points associated. Group 2 includes “1,000+” and “5,000+” - 1078 data points in total, which are marked as “XS”. Group 3 takes in “10,000+” and “50,000+” - 1187 data points, which are marked as “S”. With this method applied further, we have group 4 labeled as “M” with 1198 data points, group 5 labeled as “L” with 971 data points, and group 6 labeled as “XL” with 1108 data points. As for the single “free” one, we drop the record since it would barely affect the final result. 

After this data cleaning process, we run an ANOVA test to verify our hypothesis:

H0: There is no significant difference in ratings across the six groups with different levels of installations. 

Ha: There exists some significant differences in ratings across the six groups with different levels of installations. 


```{r}
summary(play_orig$Installs)
installsModel = aov(Rating ~ Installs, data = play)
summary(installsModel)
TukeyHSD(installsModel)
```
&nbsp;

Interpretation:

The resulting p-value appears really small, which implies that there exists a significant difference of ratings in at least one comparison pair between two groups of installation levels. We could reject the null hypothesis. 

To find out exactly which comparison pairs showcase this significance difference. We then run a Tukey’s HSD test. We could see that the p-values for 13 out of 15 comparison pairs have small p-values. The p-values for pairs XXS-XL, and XS-S are large. Plus, the p-value for pair L-XL is quite close to the 0.05 cut-off. If we set our cut-off point to 0.01, this p-value will also be considered as large. In regard to the reason why differences between pair XXS-XL is not significant, it might be due to the nature of how ratings are generated. To elaborate, when apps have fewer installations, then fewer people would rate them, and thus the ratings are determined only by relatively smaller pools of people. When apps have more installations, then more people would rate them, and therefore the extreme values of ratings could more easily be evened out. In terms of the non-significant difference between pair XS-S, we need more information in order to explain this situation. 


&nbsp;

```{r}
hist(residuals(installsModel))
```
&nbsp;

Limitation:

There are definitely limitations within our tests. One might be related to our approach of grouping different levels of installs into smaller groups. Though we have tried to have smaller groups that contain proportions of data that are as equivalent as possible, the amounts of data points included in each group still vary within a considerably large range. For instance, group “L” has 971 data points while group “M” has 1198 data points. This might affect our result to a certain extent. 

Moreover, the residuals does not appear to be normally distributed - this opposes certain assumptions that ANOVA test is based on. 


&nbsp;

&nbsp;


## Regression Model

After we get the result from T test and ANOVA, we decided to use Price, Number of reviews, Number of Installs and Paid Type to predict Rating. 

Hypothesis #1:

H0: Price does not have an impact on Ratings

H1: Price has a negative impact on Ratings


Hypothesis #2:

H0: Number of installs does not have an impact on Ratings

H1: Number of installs has a positive impact on Ratings


Hypothesis #3:

H0: Size does not have an impact on Ratings

H1: Size has a positive impact on Ratings


Hypothesis #4:

H0: Payment type of application does not have an impact on Ratings

H1: Paid application has an impact on Ratings


Hypothesis #5:

H0: Reviews does not have an impact on Ratings

H1: Reviews has a negative impact on Ratings

&nbsp;

Before we started, we build two graphs to find if there is any correlation between the variables.

```{r}
pairs(play[,4:8])
```
&nbsp;

```{r, echo=FALSE, include=FALSE}
cor.test(play$Reviews,play$size_num)
cor.test(play$Reviews,play$Price)
cor.test(play$size_num,play$Price)
summary(aov(Reviews~Type , data = play))
summary(aov(Reviews~Installs , data = play))
summary(aov(size_num~Type , data = play))
summary(aov(size_num~Installs , data = play))
summary(aov(Price~Installs , data = play))
summary(aov(Price~Type , data = play))
chisq.test(play$Installs,play$Type)
data1 <- c(1,0.22,0.08,0.002,-0.01,0.22,1,0.1,0.0001,-0.02,0.08,0.1,1,0.01,0.005,0.002,0.001,0.01,1,0.06,-0.01,-0.002,0.005,0.06,1)
mat <- matrix(data = data1,nrow = 5,ncol = 5,byrow = T)
colname1=c("reviews","size","installs","type","price")
rowname1=c("reviews","size","installs","type","price")
colnames(mat) <- colname1
rownames(mat) <- rowname1

```
&nbsp;

```{r,echo= FALSE}
library(reshape2)
data2 <- melt(mat)
head(data2)
library(ggplot2)
ggplot(data = data2, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()
```
&nbsp;


```{r}
boxplot(Rating ~ Installs, data = play)
boxplot(Rating ~ Type, data = play)
```
&nbsp;


Interpretation:

By comparing the result from the box plot, we choose the category XL and category Free as our reference. Because the values within the category is more clustered than other categories and the difference is more obvious.


&nbsp;

```{r}
play$Installs <- relevel(play$Installs, ref='XL')
xyMod2 = lm(Rating ~ Price  + Installs + Type + size_num + Reviews, data = play)
summary(xyMod2)
```

&nbsp;

Interpretation:

Our intercept has a value of 4.2. This would indicate that application with 0 in price, size, reviews and type:free and installs:L will have a score of 4.2 in rating.

The coefficient for price is -0.0008 and for size is -0.0005 which said that for each unit increase in price and size, the rating will decrease by 0.0008 and by 0.0005 respectively. The coefficient of reviews is 1.3*10^-8 and for each unit increase the rating will increase by that number. For installs size in category XXS, 0.1 tell us the change in the mean rating for someone who is in XXS relative to category XL. The category XS’s coefficient of -0.19 tell us the expected change for mean rating for someone who is in XS relative to category XL. The category S,M and L’s coefficient act the same.

The p value of price and reviews is less than 0.05 so we can reject H0 in Hypothesis 1 and Hypothesis 5. The p value of the size is larger than 0.05 so we fail to reject H0 in Hypothesis 3: Size does not have an impact on Ratings. The t value and p value of the type and installs variables are all significant because their p-value is less than 0.05. So we can reject the H0 of the Hypothesis 2 and hypothesis 4. 

The F-statistic for the whole model is 34 and that means difference in variance is significant because the corresponding p-value is less than 0.05. The “Residuals vs Fitted” plot and the “Scale-Location” plot are showing us our linear relationships within residuals and it looks a bit weird because we have categorical variables in our model but the overall fitness is fine.Finally, “Residuals vs Leverage” shows us observations exhibiting a high degree of leverage on our regression.

&nbsp;

```{r}
par(mfrow = c(2, 2))
plot(xyMod2)
hist(xyMod2$residuals)
```
&nbsp;

Limitation:

The regression model needs to be further improved for better fit because it has the ability to predict but we can’t say that this is the best predictive model. The “Normal Q-Q” plot is saying that our errors are not normally-distributed so I draw a histogram to see the distribution directly.

The adjusted R-square is really close to 0 but the r-square is based on a context and can be improved so we can’t say for sure that the model is useless. The independent variables such as price have some outliers that may impact the fitness of regression greatly. 

&nbsp;

&nbsp;


## ANOVA 2 Rating ~ Content.Rating
We are also curious about if there is any significant difference in ratings across groups of apps that are associated with different content ratings. In the original dataset there are one record with content rating left blank - this got cleaned out during our previous data preparation. Running summary on the sorted dataset, we detect there is only one record with content rating marked “unrated” - we decide to filter out this record for our following ANOVA test. Now we have content ratings categorized into five groups, which are “Adults only 18+”, “Everyone”, “Everyone 10+”, “Mature 17+” and “Teen”. 

We run an ANOVA test for our hypothesis:

H0: There does not exist any significant difference in ratings among groups with different content ratings. 

Ha: There are significant differences in ratings among the five groups with different content ratings.


```{r}
summary (play_orig$Content.Rating)
summary (play$Content.Rating)
play3 <- play %>%
  filter(Content.Rating != "Unrated")
contentModel = aov(Rating ~ Content.Rating, data = play3)
summary(contentModel)
TukeyHSD(contentModel)
```
&nbsp;

Interpretation:

The p-value from ANOVA is small - sufficient for us to reject the null. We could conclude that there exist certain significant differences in ratings among the five groups with different content ratings. To explore about exactly which comparison pairs showcase the significant difference, we run a Tukey HSD test. 

From the results of the Tukey test, we could clearly see that only the p-values for pair Teen-Everyone, Teen-Mature 17+, and Mature 17+ - Everyone 10+ are small enough to imply the significance of differences. This is based on the scenario if we set the cut-off point for p-value to 0.05. If we set the cut-off point to 0.01, the difference of average rating between the two groups in the last pair would not be considered as significant. One implication from this result could be that the main users of teen apps are teens. And most likely, whoever rate those apps are teens as well. Teens might not be as critical as older people when it comes to rating an app. They might not have that many expectations or requests as older people normally do. So the average rating for teen apps would be higher than the average ratings for both Mature 17+ and Everyone apps. This could also be supported by the fact that p-value for Teen-Mature 17+ is 0.0041228, which is slightly smaller than the p-value for Teen-Everyone. Since a majority of the users rating the Mature 17+ apps would be people over 17, it is quite reasonable that the differences between the former pair would be moderately more significant. 


Limitation:

However, if we look at the results for pairs Teen-Adults only 18+, and Teen-Everyone+, we could hardly detect similar patterns as what we just discussed. This suggests that we need to investigate more on the reasons why there are significant differences in ratings between the two aforementioned pairs, Teen-Mature 17+ and Teen-Everyone 10+. As for the pair Mature 17+ - Everyone 10+, we also need more information in order to thoroughly understand the result. These imply the complexity of the reasons why ratings differentiate among certain groups with different content ratings. These also hint on the limitations of the original data set when we try to delve deeper into the issue. 


&nbsp;

&nbsp;



## ANOVA 3 Rating ~ Category

We want to test if there is a difference in average rating among different categories so we first run a anova test for the category. There are 33 levels of category and the resulting p-value is small, which means that there is a significant difference in at least one of the category comparisons. We then run a Tukey test and found that most of the comparisons among two categories have big p-adjusted, meaning that the differences are insignificant. 

In real life, in order to make informational decisions, people usually start from the highest level and then go into details for more exploration. And since we already identified that there is at least one significant difference across categories, we want to know that from a higher level, the rating of which broader group is most distinct from all others. Based on our understanding of the apps and personal experiences, we manually created four broader groups: Entertainment, Education, Tool, and Living. We then conducted a ANOVA and Tukey test to test our hypothesis.

&nbsp;

Hypothesis:

H0: the average rating is the same across all four groups

Ha: the average rating is not the same across all four groups

```{r}
play2 <- play

play2$Category = as.character(play2$Category)

play2$Category[play2$Category == "ART_AND_DESIGN" | play2$Category == "SPORTS" | play2$Category == "COMICS" | play2$Category == "ENTERTAINMENT" | play2$Category == "GAME" | play2$Category == "SPORTS" | play2$Category == "SHOPPING" | play2$Category == "TRAVEL_AND_LOCAL" | play2$Category == "VIDEO_PLAYERS" | play2$Category == "SOCIAL" | play2$Category == "PHOTOGRAPHY" | play2$Category == "PERSONALIZATION" | play2$Category == "DATING" | play2$Category == "BEAUTY" | play2$Category == "AUTO_AND_VEHICLES"] <- "ENTERTAINMENT"

play2$Category[play2$Category == "BOOKS_AND_REFERENCE" | play2$Category == "NEWS_AND_MAGAZINES" | play2$Category == "EDUCATION" | play2$Category == "BUSINESS" | play2$Category == "PRODUCTIVITY" | play2$Category == "FINANCE" | play2$Category == "LIBRARIES_AND_DEMO"] <- "EDUCATION"

play2$Category[play2$Category == "TOOLS" | play2$Category == "MAPS_AND_NAVIGATION" | play2$Category == "COMMUNICATION"] <-  "TOOL"

play2$Category[play2$Category == "HOUSE_AND_HOME" | play2$Category == "PARENTING" | play2$Category == "MEDICAL" | play2$Category == "FAMILY" | play2$Category == "LIFESTYLE" | play2$Category == "WEATHER" | play2$Category == "EVENTS" | play2$Category == "FOOD_AND_DRINK" | play2$Category == "HEALTH_AND_FITNESS"] <-  "LIVING"

```

&nbsp;

```{r}
categoryModel = aov(Rating ~ Category, data = play2)
summary(categoryModel)
TukeyHSD(categoryModel)
```
&nbsp;

Interpretation:

Based on our results, we identified that TOOL category overall has the most significant difference from the other three categories and its apps have the lowest average rating. We believe that one of the reasons for this pattern is that tools (which includes communication and maps & navigation) are the most commonly used apps on a daily basis so people generally will be more critical and have higher expectations on the functionality of these apps.


&nbsp;

```{r,,warning=FALSE,message=FALSE}
hist(residuals(categoryModel))

library(car)
leveneTest(Rating ~ Category, data = play)
```

Limitation:

Our classification of the categories is highly biased and different methods of classification might lead to different results. 

Additionally, there are multiple assumptions on One-Way Anova test: errors are independent, errors are normally distributed, variances are homogeneous, and additivity of model effects. We ran some examinations to test the second and third assumptions. From the residual histogram, we can see that the residuals are not normally distributed and the distribution is skewed to the left, which means that the normality assumption in this analysis is not met. Furthermore, the p-value resulted from the Levene's Test is significant. Thus the null hypothesis is rejected and we concluded that the variances are not homogeneous. The failure to meet the assumptions of Anova test is one of the limitations for this analysis.


&nbsp;

&nbsp;


